{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a9ugRZCMqGF8"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers.activation import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-TTVnTZaYek",
    "outputId": "7704806e-74ae-4506-d1de-36e30c4a4e18"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "qw6NIoFuqZuZ",
    "outputId": "d3afdcaa-b459-437b-d9bd-1592a786e68b"
   },
   "outputs": [],
   "source": [
    "training_data = np.load('../datasets/full_numpy_bitmap_smiley face.npy')\n",
    "# training_data = np.array([np.array_split(img_arr, 28) for img_arr in training_data])\n",
    "training_data = training_data.reshape(training_data.shape[0], 28, 28).astype(\n",
    "    \"float32\")\n",
    "X_train = training_data\n",
    "print(X_train[0].shape)\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HA2ByAaAqrzZ",
    "outputId": "278e6ec8-b21e-4e6b-afbf-d9e6cd91353e"
   },
   "outputs": [],
   "source": [
    "#Define input image dimensions\n",
    "#Large images take too much time and resources.\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "print(img_shape)\n",
    "noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
    "print(noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCC3Cd-BrKW4",
    "outputId": "82dd4ce2-549a-426c-9230-5b502c3e2e07",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(256 * 7 * 7, input_shape=(100,)))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(Reshape((7, 7, 256)))\n",
    "    generator.add(Conv2DTranspose(128, kernel_size=5, strides=1, padding=\"same\"))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(Conv2DTranspose(64, kernel_size=3, strides=2,padding=\"same\"))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\", activation='tanh'))\n",
    "    generator.summary()\n",
    "    noise = Input(shape=(100,))\n",
    "    fake_image = generator(noise)\n",
    "    return Model(inputs=noise, outputs=fake_image)\n",
    "\n",
    "def build_discriminator():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Conv2D(128, kernel_size=5, strides=2, input_shape=(28, 28, 1), padding=\"same\"))\n",
    "    discriminator.add(LeakyReLU())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    discriminator.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    discriminator.add(LeakyReLU())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    discriminator.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    discriminator.add(LeakyReLU())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.summary()\n",
    "    img = Input(shape=(28, 28, 1))\n",
    "    probability = discriminator(img)\n",
    "    return Model(inputs=img, outputs=probability)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"gen_images/epoch_%d.png\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "def train(epochs, batch_size=128, save_interval=100):\n",
    "    training_data = np.load('../datasets/full_numpy_bitmap_smiley face.npy')\n",
    "    # training_data = np.array([np.array_split(img_arr, 28) for img_arr in training_data])\n",
    "    training_data = training_data.reshape(training_data.shape[0], 28, 28)\n",
    "    X_train = training_data\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3) \n",
    "    half_batch = int(batch_size / 2)\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
    "        noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "        valid_y = np.array([1] * batch_size)\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)     \n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "\n",
    "optimizer = Adam(0.001, 0.5)\n",
    "disc_optimizer = RMSprop(0.005)\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)     \n",
    "discriminator.trainable = False  \n",
    "valid = discriminator(img) \n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=10000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.save('saved_models/smiley_gan_h5_v1', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "loaded_combined = load_model('saved_models/smiley_gan_h5')\n",
    "loaded_generator = loaded_combined.get_layer('model_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x249a9d75e40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, (1, 100))\n",
    "gen_imgs = loaded_generator.predict(noise)\n",
    "plt.imshow(gen_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_CIFAR10_01July2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
